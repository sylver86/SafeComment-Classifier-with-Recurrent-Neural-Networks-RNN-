{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1eac10",
   "metadata": {
    "id": "1f1eac10"
   },
   "source": [
    "## Progetto : Protezione e Filtro da Commenti Dannosi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504d2cc",
   "metadata": {
    "id": "3504d2cc"
   },
   "source": [
    "**Descrizione**\n",
    "\n",
    "Questo progetto mira a creare un ambiente online più sicuro e rispettoso attraverso l'identificazione e la filtrazione di commenti potenzialmente dannosi. Utilizzando tecniche di Deep Learning avanzate, sviluppiamo un modello in grado di analizzare i commenti degli utenti in tempo reale, classificandoli in base al grado di dannosità del linguaggio. Questo sistema multilabel di classificazione fornisce una valutazione dettagliata del contenuto testuale, contribuendo significativamente alla protezione da linguaggi tossici, osceni, minacciosi, insultanti e discriminatori.\n",
    "<br><br>\n",
    "**Punti Chiave del Progetto**\n",
    "\n",
    "1. Preprocessamento del Testo: Implementazione di tecniche avanzate per eliminare token non significativi, migliorando l'accuratezza della classificazione rimuovendo elementi che non contribuiscono al significato semantico del testo.\n",
    "\n",
    "2. Trasformazione in Sequenze: Conversione del testo preprocessato in sequenze, preparando i dati per un'analisi approfondita che cattura le dinamiche e le relazioni semantiche all'interno del corpus testuale.\n",
    "  \n",
    "3. Modello di Deep Learning: Costruzione di un modello con layer ricorrenti, ottimizzato per il task di classificazione multilabel, in grado di interpretare la complessità e le sfumature del linguaggio naturale.\n",
    "   \n",
    "4. Classificazione in Tempo Reale: In fase di predizione, il modello valuta i commenti fornendo un vettore di classificazione: un commento viene considerato non dannoso se il vettore risultante è composto da zeri [0,0,0,0,0,0], altrimenti, la presenza di almeno un \"1\" indica la categoria di dannosità rilevata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8529ff-a84d-489b-9f5e-98e0b5f9ee5f",
   "metadata": {},
   "source": [
    "# 1. Il Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f480106-0e4d-4a43-b85c-294985e5c173",
   "metadata": {},
   "source": [
    "Il progetto utilizza un dataset, disponibile in un file CSV, che include una raccolta di commenti. Ogni commento nel dataset è associato a una valutazione che ne indica il livello di dannosità. <br>Inizieremo scaricando questo dataset per poi procedere con la sua elaborazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10df25a-83cf-44dd-b2ec-b1628808ca23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f2fb60",
   "metadata": {
    "id": "c2f2fb60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "BASE_URL = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/\"\n",
    "df = pd.read_csv(BASE_URL+\"Filter_Toxic_Comments_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c3d701",
   "metadata": {
    "id": "66c3d701",
    "outputId": "e08b454a-afe2-4b45-caf6-bd0d5ffd41a0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum_injurious</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  sum_injurious  \n",
       "0        0       0       0              0              0  \n",
       "1        0       0       0              0              0  \n",
       "2        0       0       0              0              0  \n",
       "3        0       0       0              0              0  \n",
       "4        0       0       0              0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1c65f-2278-497b-b154-84bd832bf1ff",
   "metadata": {},
   "source": [
    "Nel dataset, troviamo una gamma di commenti che varia dai contenuti esplicitamente negativi a quelli neutrali o completamente privi di elementi dannosi. Importante per la nostra analisi è la presenza di un'etichetta per ogni commento: questa etichetta assume valore 1 per indicare un commento dannoso in relazione alla specifica categoria di dannosità menzionata nella colonna corrispondente. In aggiunta, il dataset include una colonna denominata \"sum_injurious\", la quale rappresenta la somma delle valutazioni di dannosità attribuite a ogni commento, offrendo una panoramica complessiva del livello di negatività espresso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff7479-c2fa-41e6-a5a2-a2863a83fbd4",
   "metadata": {},
   "source": [
    "Se desideriamo individuare i commenti che presentano due specifiche tipologie di dannosità, possiamo eseguire un'operazione di filtraggio sui dati. <br>Questo ci permette di selezionare solo quei commenti che soddisfano il nostro criterio specifico, ovvero avere due valutazioni di dannosità quindi ad esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e65ce3",
   "metadata": {
    "id": "19e65ce3",
    "outputId": "d0c5f387-3531-4923-dddb-8c41f31ff081"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum_injurious</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>My Band Page's deletion. You thought I was gon...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Hi! I am back again!\\nLast warning!\\nStop undo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Would you both shut up, you don't run wikipedi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>You should be fired, you're a moronic wimp who...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159253</th>\n",
       "      <td>what do you mean \\n\\nwhy don't you keep your n...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159334</th>\n",
       "      <td>Horse's ass \\n\\nSeriously, dude, what's that h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159449</th>\n",
       "      <td>I think he is a gay fag!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159514</th>\n",
       "      <td>YOU ARE A MISCHIEVIOUS PUBIC HAIR</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159546</th>\n",
       "      <td>\"\\n\\nHey listen don't you ever!!!! Delete my e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3480 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "51      GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...      1   \n",
       "58      My Band Page's deletion. You thought I was gon...      1   \n",
       "79      Hi! I am back again!\\nLast warning!\\nStop undo...      1   \n",
       "86      Would you both shut up, you don't run wikipedi...      1   \n",
       "168     You should be fired, you're a moronic wimp who...      1   \n",
       "...                                                   ...    ...   \n",
       "159253  what do you mean \\n\\nwhy don't you keep your n...      1   \n",
       "159334  Horse's ass \\n\\nSeriously, dude, what's that h...      1   \n",
       "159449                         I think he is a gay fag!!!      1   \n",
       "159514                  YOU ARE A MISCHIEVIOUS PUBIC HAIR      1   \n",
       "159546  \"\\n\\nHey listen don't you ever!!!! Delete my e...      1   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  sum_injurious  \n",
       "51                 0        1       0       0              0              2  \n",
       "58                 0        1       0       0              0              2  \n",
       "79                 0        0       1       0              0              2  \n",
       "86                 0        0       0       1              0              2  \n",
       "168                0        0       0       1              0              2  \n",
       "...              ...      ...     ...     ...            ...            ...  \n",
       "159253             0        1       0       0              0              2  \n",
       "159334             0        1       0       0              0              2  \n",
       "159449             0        0       0       0              1              2  \n",
       "159514             0        0       0       1              0              2  \n",
       "159546             0        0       0       1              0              2  \n",
       "\n",
       "[3480 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sum_injurious']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79b88a-1a04-4489-bacc-82838b47cc99",
   "metadata": {},
   "source": [
    "Il totale dei commenti sono :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b21e249",
   "metadata": {
    "id": "6b21e249",
    "outputId": "32fb9c3b-c21e-4237-c6f1-9c4c3a992182"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfcff5-0830-44bf-b8e8-7a22513f14d8",
   "metadata": {
    "id": "6a6421ed"
   },
   "source": [
    "# 2. Preprocessing del Testo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f32ab-1a94-4cdc-a7f1-712e86ee81d0",
   "metadata": {},
   "source": [
    "Inizieremo a preparare il testo contenuto nei commenti, rimuovendo gli elementi che non aggiungono valore significativo al significato complessivo del testo. Questo processo di pulizia è fondamentale per aiutare la Rete Neurale a imparare in modo più efficace, concentrandosi solo sulle parti del testo che contribuiscono realmente al contesto semantico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81053fa6-63d7-4008-818c-cc4cfbfb7b83",
   "metadata": {},
   "source": [
    "Per fare questo andremo per step nello specifico eliminando innanzitutto le parole chiamate \"stop words\", effettuando la Lemmatizzazione, ed infine la rimozione di caratteri speciali e punteggiatura. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7e422-0211-4524-810b-b15973ddfac2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2a. Rimozione delle Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ff1ee-0ee0-4de4-9fbe-bfd25ee9c03f",
   "metadata": {},
   "source": [
    "Procediamo ora alla rimozione di eventuali \"Stop Words\" ovvero parole molto comuni nella lingua considerata, come articoli, preposizioni, congiunzioni, e alcune forme verbali ausiliarie, che non aggiungono significato significativo al testo per molti task di Natural Language Processing (NLP). L'idea è che, rimuovendo queste parole, sia possibile concentrarsi sulle parole che hanno più importanza e significato nel contesto di un'analisi specifica, come l'analisi del sentimento, la classificazione dei documenti, il riconoscimento di entità nominate, ecc.\n",
    "\n",
    "Per esempio, in inglese, parole come \"the\", \"is\", \"at\", \"which\", e \"on\" sono spesso considerate stop words, dato che compaiono molto frequentemente e di solito non portano informazioni critiche per comprendere il significato di un testo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46989429-ec62-49ba-9469-1bd51831a66d",
   "metadata": {},
   "source": [
    "Per procedere con questo step andiamo a utilizzare la libreria \"nltk\" utile a questo scopo. <br>\n",
    "Con il codice sotto abbiamo ricevuto grazie a questa libreria un elenco di parole che possiamo rimuovere dai nostri commenti e che fanno riferimento alle Stop Words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e27784-0ec7-445e-8f68-429265953e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eugenix/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Carica le stop words predefinite per l'inglese\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ffd60-3206-442f-802f-dd6880add3bf",
   "metadata": {},
   "source": [
    "Procediamo dunque con l'eliminazione dalla lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b36f3f-337f-4000-8d31-80eeca236940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum_injurious</th>\n",
       "      <th>comment_text_filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation edits made username Hardcore Metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww ! matches background colour 'm seemingly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man , 'm really trying edit war . 's guy c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` ca n't make real suggestions improvement - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>, sir , hero . chance remember page 's ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` : : : : : second time asking , view complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ashamed horrible thing put talk page . 128.61....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer Umm , theres actual article prostituti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` ... really n't think understand . came idea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  sum_injurious  \\\n",
       "0                  0        0       0       0              0              0   \n",
       "1                  0        0       0       0              0              0   \n",
       "2                  0        0       0       0              0              0   \n",
       "3                  0        0       0       0              0              0   \n",
       "4                  0        0       0       0              0              0   \n",
       "...              ...      ...     ...     ...            ...            ...   \n",
       "159566             0        0       0       0              0              0   \n",
       "159567             0        0       0       0              0              0   \n",
       "159568             0        0       0       0              0              0   \n",
       "159569             0        0       0       0              0              0   \n",
       "159570             0        0       0       0              0              0   \n",
       "\n",
       "                                      comment_text_filter  \n",
       "0       Explanation edits made username Hardcore Metal...  \n",
       "1       D'aww ! matches background colour 'm seemingly...  \n",
       "2       Hey man , 'm really trying edit war . 's guy c...  \n",
       "3       `` ca n't make real suggestions improvement - ...  \n",
       "4                , sir , hero . chance remember page 's ?  \n",
       "...                                                   ...  \n",
       "159566  `` : : : : : second time asking , view complet...  \n",
       "159567  ashamed horrible thing put talk page . 128.61....  \n",
       "159568  Spitzer Umm , theres actual article prostituti...  \n",
       "159569  looks like actually put speedy first version d...  \n",
       "159570  `` ... really n't think understand . came idea...  \n",
       "\n",
       "[159571 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Definiamo un metodo che pulisce le parole\n",
    "def delete_stop(text):\n",
    "    tokens = word_tokenize(text) # Tokenizziamo la frase\n",
    "    word_filter = [elem for elem in tokens if elem.lower() not in stop_words] #Filtriamo i token che rappresentano le Stopwords\n",
    "    word_filter_sentence = ' '.join(word_filter) # Ricompongo in unica frase i token rimasti.\n",
    "    return word_filter_sentence\n",
    "    \n",
    "# Applichiamo tale metodo in tutto il dataframe\n",
    "\n",
    "df['comment_text_filter'] = df['comment_text'].apply(delete_stop)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05f858-e352-4fe6-adc5-76f2342ef47e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2b. La Lemmatizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652e6f3-c876-4688-88a9-f58a4b3c90af",
   "metadata": {},
   "source": [
    "Una volta effettuata la rimozione delle stop words, procederemo ad effettuare un ulteriore passaggio di Data Cleaning chiamato \"Lemmatizzazione\".<br>\n",
    "La lemmatizzazione è il processo di riduzione delle parole a una forma base o lemma, tenendo conto del loro significato e della loro funzione grammaticale nella frase. <br>\n",
    "Per far questo utilizzeremo una libreria di Python chiamata \"nltk\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a89b51-e2af-4b3b-9cbc-03318b64d5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eugenix/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eugenix/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eugenix/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum_injurious</th>\n",
       "      <th>comment_text_filter</th>\n",
       "      <th>comment_text_filter_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation edits made username Hardcore Metal...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww ! matches background colour 'm seemingly...</td>\n",
       "      <td>d'aww ! match background colour 'm seemingly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man , 'm really trying edit war . 's guy c...</td>\n",
       "      <td>hey man , 'm really trying edit war . 's guy c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` ca n't make real suggestions improvement - ...</td>\n",
       "      <td>`` ca n't make real suggestion improvement - w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>, sir , hero . chance remember page 's ?</td>\n",
       "      <td>, sir , hero . chance remember page 's ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` : : : : : second time asking , view complet...</td>\n",
       "      <td>`` : : : : : second time asking , view complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ashamed horrible thing put talk page . 128.61....</td>\n",
       "      <td>ashamed horrible thing put talk page . 128.61....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer Umm , theres actual article prostituti...</td>\n",
       "      <td>spitzer umm , there actual article prostitutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` ... really n't think understand . came idea...</td>\n",
       "      <td>`` ... really n't think understand . came idea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  sum_injurious  \\\n",
       "0                  0        0       0       0              0              0   \n",
       "1                  0        0       0       0              0              0   \n",
       "2                  0        0       0       0              0              0   \n",
       "3                  0        0       0       0              0              0   \n",
       "4                  0        0       0       0              0              0   \n",
       "...              ...      ...     ...     ...            ...            ...   \n",
       "159566             0        0       0       0              0              0   \n",
       "159567             0        0       0       0              0              0   \n",
       "159568             0        0       0       0              0              0   \n",
       "159569             0        0       0       0              0              0   \n",
       "159570             0        0       0       0              0              0   \n",
       "\n",
       "                                      comment_text_filter  \\\n",
       "0       Explanation edits made username Hardcore Metal...   \n",
       "1       D'aww ! matches background colour 'm seemingly...   \n",
       "2       Hey man , 'm really trying edit war . 's guy c...   \n",
       "3       `` ca n't make real suggestions improvement - ...   \n",
       "4                , sir , hero . chance remember page 's ?   \n",
       "...                                                   ...   \n",
       "159566  `` : : : : : second time asking , view complet...   \n",
       "159567  ashamed horrible thing put talk page . 128.61....   \n",
       "159568  Spitzer Umm , theres actual article prostituti...   \n",
       "159569  looks like actually put speedy first version d...   \n",
       "159570  `` ... really n't think understand . came idea...   \n",
       "\n",
       "                           comment_text_filter_lemmatized  \n",
       "0       explanation edits made username hardcore metal...  \n",
       "1       d'aww ! match background colour 'm seemingly s...  \n",
       "2       hey man , 'm really trying edit war . 's guy c...  \n",
       "3       `` ca n't make real suggestion improvement - w...  \n",
       "4                , sir , hero . chance remember page 's ?  \n",
       "...                                                   ...  \n",
       "159566  `` : : : : : second time asking , view complet...  \n",
       "159567  ashamed horrible thing put talk page . 128.61....  \n",
       "159568  spitzer umm , there actual article prostitutio...  \n",
       "159569  look like actually put speedy first version de...  \n",
       "159570  `` ... really n't think understand . came idea...  \n",
       "\n",
       "[159571 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Assicurati di aver scaricato le risorse necessarie da NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inizializza il lemmatizzatore\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Applica la lemmatizzazione ai token \n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens]\n",
    "    \n",
    "    # Ricrea la frase unendo i token lemmatizzati\n",
    "    lemmatized_sentence = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_sentence\n",
    "    \n",
    "df['comment_text_filter_lemmatized'] = df['comment_text_filter'].apply(lemmatize_text)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ddb158-ff42-4b67-8632-f72ccb708eea",
   "metadata": {},
   "source": [
    "## 2c. Rimozione caratteri speciali e punteggiatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa715f1-3cbe-4da7-b910-b3427ad705c3",
   "metadata": {},
   "source": [
    "Per il nostro progetto, svilupperemo una funzione specifica che si occupa di rimuovere la punteggiatura e i caratteri speciali dal testo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a939be36-5389-445e-9e66-19730499f116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Definizione della funzione per rimuovere la punteggiatura da una stringa di testo\n",
    "def remove_punctuation(text):\n",
    "    # Usa una comprehension list per mantenere solo i caratteri non presenti in string.punctuation\n",
    "    token_no_punct = [char for char in text if char not in string.punctuation]\n",
    "    # Unisce i caratteri rimanenti in una stringa\n",
    "    words_wo_punct = ''.join(token_no_punct)\n",
    "    return words_wo_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f296b9-d49e-45ed-9db7-d9fd8df8c475",
   "metadata": {},
   "source": [
    "Andiamo ad eseguirla sui nostri commenti :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52a7e5b-9b36-48ed-9122-fb4b6c3b88dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum_injurious</th>\n",
       "      <th>comment_text_filter</th>\n",
       "      <th>comment_text_filter_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation edits made username Hardcore Metal...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww ! matches background colour 'm seemingly...</td>\n",
       "      <td>daww  match background colour m seemingly stuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man , 'm really trying edit war . 's guy c...</td>\n",
       "      <td>hey man  m really trying edit war  s guy const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` ca n't make real suggestions improvement - ...</td>\n",
       "      <td>ca nt make real suggestion improvement  wonde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>, sir , hero . chance remember page 's ?</td>\n",
       "      <td>sir  hero  chance remember page s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` : : : : : second time asking , view complet...</td>\n",
       "      <td>second time asking  view completely cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ashamed horrible thing put talk page . 128.61....</td>\n",
       "      <td>ashamed horrible thing put talk page  128611993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer Umm , theres actual article prostituti...</td>\n",
       "      <td>spitzer umm  there actual article prostitution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` ... really n't think understand . came idea...</td>\n",
       "      <td>really nt think understand  came idea bad ri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  sum_injurious  \\\n",
       "0                  0        0       0       0              0              0   \n",
       "1                  0        0       0       0              0              0   \n",
       "2                  0        0       0       0              0              0   \n",
       "3                  0        0       0       0              0              0   \n",
       "4                  0        0       0       0              0              0   \n",
       "...              ...      ...     ...     ...            ...            ...   \n",
       "159566             0        0       0       0              0              0   \n",
       "159567             0        0       0       0              0              0   \n",
       "159568             0        0       0       0              0              0   \n",
       "159569             0        0       0       0              0              0   \n",
       "159570             0        0       0       0              0              0   \n",
       "\n",
       "                                      comment_text_filter  \\\n",
       "0       Explanation edits made username Hardcore Metal...   \n",
       "1       D'aww ! matches background colour 'm seemingly...   \n",
       "2       Hey man , 'm really trying edit war . 's guy c...   \n",
       "3       `` ca n't make real suggestions improvement - ...   \n",
       "4                , sir , hero . chance remember page 's ?   \n",
       "...                                                   ...   \n",
       "159566  `` : : : : : second time asking , view complet...   \n",
       "159567  ashamed horrible thing put talk page . 128.61....   \n",
       "159568  Spitzer Umm , theres actual article prostituti...   \n",
       "159569  looks like actually put speedy first version d...   \n",
       "159570  `` ... really n't think understand . came idea...   \n",
       "\n",
       "                           comment_text_filter_lemmatized  \n",
       "0       explanation edits made username hardcore metal...  \n",
       "1       daww  match background colour m seemingly stuc...  \n",
       "2       hey man  m really trying edit war  s guy const...  \n",
       "3        ca nt make real suggestion improvement  wonde...  \n",
       "4                      sir  hero  chance remember page s   \n",
       "...                                                   ...  \n",
       "159566        second time asking  view completely cont...  \n",
       "159567    ashamed horrible thing put talk page  128611993  \n",
       "159568  spitzer umm  there actual article prostitution...  \n",
       "159569  look like actually put speedy first version de...  \n",
       "159570    really nt think understand  came idea bad ri...  \n",
       "\n",
       "[159571 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_text_filter_lemmatized'] = df['comment_text_filter_lemmatized'].apply(remove_punctuation)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37423f1e-20be-488c-bf3e-331439bef426",
   "metadata": {},
   "source": [
    "# 3. Trasformazione in Sequenze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e714d-2e50-48eb-a19f-2b774afb3828",
   "metadata": {},
   "source": [
    "Una volta completato il preprocessing dei dati, possiamo trasformare le liste di token in sequenze numeriche tramite il Tokenizer di Keras. Questa operazione ci permette di preparare i dati in modo adeguato per l'addestramento di modelli di deep learning, i quali necessitano di input in forma numerica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd682a6c-38e4-420f-b74e-e058cbeaacdc",
   "metadata": {},
   "source": [
    "Prima pero' di procedere andiamo a segmentare il dataset in componente train e componente di test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74742435-0008-4e2f-b3b9-2a9bed726381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separazione delle features e delle etichette\n",
    "X = df['comment_text_filter_lemmatized'].tolist()\n",
    "y = df[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "\n",
    "# Divisione in set di addestramento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87d82b-4e38-4bbc-a174-fe9bb0ddb7b4",
   "metadata": {},
   "source": [
    "Ora, trasformeremo `x_train` e `x_test` in sequenze numeriche, sfruttando il Tokenizer di Keras precedentemente addestrato sui dati contenuti in `x_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d2915f-6ebf-442e-896a-cd8660d11beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7499, 3114, 3799, 7499, 3799, 267, 7499, 964, 1749, 696], [1034, 23, 729, 104, 7, 6671, 1290, 334, 454, 3661, 6069, 7, 1595, 2496, 273, 610, 169, 7309, 33, 494, 7, 148, 1152, 183, 860, 50, 2048, 4722, 17, 19, 1, 88, 1479, 8080, 69, 81, 56, 539, 9526], [5081, 57, 6594, 6002, 176, 2814, 811, 42, 5630, 1430, 220, 140, 1338, 3248, 4653, 3495, 2461, 637, 1338, 1725, 2156, 5630, 5837, 6521, 2377, 1283, 6445, 1962, 617, 3248, 4653, 3495, 2461, 4823, 389, 6070, 583, 1962, 396, 637, 1338, 1725, 316, 5630, 349, 1962, 155, 57, 922, 474, 60, 2168, 2014, 879, 1233, 5081, 617, 262, 382, 2344, 51, 814, 237, 1962, 1622, 5035, 57, 498, 154, 1826, 35, 5197, 1551, 42, 317, 312, 1430, 353, 6905, 6445, 1962, 474, 6192, 94, 6975, 5630, 1962, 120, 129, 9384, 8636, 1, 57, 240, 770, 2469, 9220, 6975, 299, 23, 2278, 2250, 164, 1129, 5837, 2377, 1962, 929, 48, 2344, 51, 10, 814, 237, 1962, 131, 5837, 353, 389, 804, 4125, 1980, 396, 617, 1129, 5837, 454, 1399, 1085, 929, 48, 1261, 12, 5890, 4393, 91, 546, 498, 1129, 6521, 712, 4125, 2801, 1749, 929, 48, 1282, 694, 2344, 179, 12, 91, 1129, 923, 93, 1744, 220, 220, 220, 88, 389, 4125, 396, 2362, 2867, 923, 1589, 25, 1367, 3815, 578, 95, 2920, 3927, 923, 1802, 1226, 23, 88, 389, 4125, 396, 3389, 578, 1096, 1129, 8774, 389, 823, 396, 48, 2362, 5837, 1431, 1367, 5837, 220, 1079, 1726, 409, 602, 2920, 8774, 2112, 602, 911, 157, 3888, 2912, 206, 8, 5837, 220, 4972, 1745, 151, 1077, 4879, 2161, 3516, 552, 573, 5784, 2836, 1652, 102, 2103, 619, 920, 2057, 8774, 2311, 823, 379, 662, 376, 815, 246, 2316, 8774, 2502, 962, 4289, 602, 656, 964, 6975, 1670, 1638, 6306, 474, 7401, 5630, 1962, 746, 3445, 57, 3227, 1962, 1528, 1129, 5630, 1962, 5630, 410, 663, 4126, 29, 154, 1261, 12, 1166, 9, 97, 273, 1962, 143, 2365, 153, 410, 663, 1189, 5837, 53, 976, 5630, 410, 5630, 389, 5247, 5837, 1315, 4125, 674, 115, 5495, 3217, 396, 1887, 389, 410, 4125, 3474, 396, 619, 389, 5495, 1962, 2256, 489, 2802, 826, 326, 2497, 192, 61, 396, 5495, 1962, 619, 389, 5198, 272, 262, 1475, 3928, 220, 2790, 636, 4227, 2802, 1422, 1646, 2802, 7402, 663, 4126, 466, 7891, 8349, 2212, 323, 217, 396, 1282, 3928, 185, 449, 4228, 48, 1105, 179, 911, 12, 5360, 1318, 389, 482, 1058, 1744, 220, 4461, 155, 619, 6976, 955, 5837, 3160, 728, 413, 955, 770, 46, 299, 3569, 2352, 5198, 663, 4126, 1783, 380, 5593, 5198, 396], [4851, 4851], [309, 712, 6, 4, 1382, 747, 367, 587, 367, 102, 2227, 1347, 184, 21, 916, 1967, 1172, 61, 1203, 9385]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Inizializza il Tokenizer\n",
    "# num_words è il numero massimo di parole da tenere, basato sulla frequenza delle parole.\n",
    "# Ogni parola riceverà un ID univoco. Solo le parole più comuni num_words verranno mantenute.\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "# Addestra il Tokenizer sulle liste di token del train\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Converti le liste di token in sequenze numeriche (per test e train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test  =  tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Stampa solo le prime 5 sequenze per verificare\n",
    "print(sequences_train[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01dda3-b3ee-4b23-806d-fae69bc8db0a",
   "metadata": {},
   "source": [
    "Andiamo a definirci la dimensione del vocabolario ottenuto tramite il Tokenizer attraverso la len() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542f832f-0e76-4c11-ba37-c007d1d59bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193930"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_index)+1\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0c3dd-0c0e-4f45-9035-f947b79e21a0",
   "metadata": {},
   "source": [
    "Adesso procederemo con la standardizzazione delle sequenze generate per i dati di addestramento e test, applicando il **\"Padding\"**. <br>Questo processo uniformerà la lunghezza di tutte le sequenze, impostandola pari alla lunghezza massima trovata nelle sequenze di addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0757ef0-6d31-4de0-a19b-e1203bdd8d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Mi elaboro la lunghezza delle sequence train (e le uso anche per il test come fatto per tokenizer prima)\n",
    "maxlen = max(sequences_train, key=len) \n",
    "len_max_len = len(maxlen)\n",
    "\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = len_max_len, padding='pre')  \n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = len_max_len, padding='pre')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3432e-df5d-45ef-bbf4-e6180a1df479",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Implementazione Rete RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ec2f4-48fb-4be9-8fd1-6385a00bb8b3",
   "metadata": {},
   "source": [
    "Come abbiamo visto nel dataframe noi abbiamo un tot. di categorie che classificano la frase rivediamolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15841796-7fe5-4881-a25c-e66bc9094b24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0                  0        0       0       0              0  \n",
       "1                  0        0       0       0              0  \n",
       "2                  0        0       0       0              0  \n",
       "3                  0        0       0       0              0  \n",
       "4                  0        0       0       0              0  \n",
       "...              ...      ...     ...     ...            ...  \n",
       "159566             0        0       0       0              0  \n",
       "159567             0        0       0       0              0  \n",
       "159568             0        0       0       0              0  \n",
       "159569             0        0       0       0              0  \n",
       "159570             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['comment_text','toxic','severe_toxic','obscene','threat','insult','identity_hate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25084df3-a4ab-41f1-97eb-958836cd3a32",
   "metadata": {},
   "source": [
    "Essendoci sei categorie che definiscono la classificazione dei commenti, la nostra Rete Neurale prevederà un output di sei neuroni. Per il layer di output, utilizzeremo la funzione di attivazione Softmax, che è adatta per la classificazione multiclasse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde2a24-9ee8-41c9-b0d8-58d712d2ca58",
   "metadata": {},
   "source": [
    "Integreremo quindi uno strato di Embedding nella struttura della nostra rete, impostando come dimensione di input il \"vocabulary size\" ottenuto dal nostro tokenizer. Aggiungeremo poi uno strato LSTM, elemento chiave della nostra Rete Neurale Ricorrente, che elaborerà sequenzialmente l'intero insieme di dati. Concluderemo con uno strato Dense di output, utilizzando la funzione di attivazione softmax, per classificare i dati nelle sei categorie specifiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34048cb4-deff-4f91-b6f5-7e2f48f38ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb0944c5-0cf6-421c-bfb1-65b3b92455f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b376d42-3569-4b2e-888f-109b87143687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1250, 128)         24823040  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24872838 (94.88 MB)\n",
      "Trainable params: 24872838 (94.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(Embedding(vocabulary_size, 128, input_length=len_max_len))\n",
    "model_LSTM.add(LSTM(64, activation='tanh'))\n",
    "model_LSTM.add(Dense(6, activation='softmax'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9c2cc-ab18-471d-9a8d-7ca75c2c7a91",
   "metadata": {},
   "source": [
    "Abbiamo definito quindi un architettura fatta da un Layer Embedding che riceverà in ingresso un numero pari al numero di parole definite nel dizionario del tokenizer addestrato sul dataset di train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48239318-d4af-4a13-bd97-8d486395d93d",
   "metadata": {},
   "source": [
    "Nel Layer successivo andiamo a definire la nostra Rete neurale ricorrente tramite il Layer LSTM che dispone di 64 neuroni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde07abe-098c-42bd-aa47-27a573bc6367",
   "metadata": {},
   "source": [
    "Infine il layer Denso che mi consente la multi classificazione nelle 6 classi previste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d5b1e-4aa8-4daa-9c2c-64a693b339e5",
   "metadata": {},
   "source": [
    "Andiamo ora a compilare e ad addestrare la nostra Rete Neurale Ricorrente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a6bcaea-d7c9-418f-ac5b-80f49f53bdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_LSTM.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a86bc2e6-c131-43d4-b50e-75fbae49e361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3192/3192 [==============================] - 1845s 578ms/step - loss: 0.0913 - accuracy: 0.9901 - val_loss: 0.0597 - val_accuracy: 0.9943\n",
      "Epoch 2/3\n",
      "3192/3192 [==============================] - 1839s 576ms/step - loss: 0.0537 - accuracy: 0.9941 - val_loss: 0.0546 - val_accuracy: 0.9943\n",
      "Epoch 3/3\n",
      "3192/3192 [==============================] - 1839s 576ms/step - loss: 0.0508 - accuracy: 0.9941 - val_loss: 0.0533 - val_accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x78adb04d1310>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM.fit(padded_sequences_train, y_train, validation_split=0.2, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d5ac4-aa03-4e91-94da-8d105d12315e",
   "metadata": {},
   "source": [
    "Procediamo infine a fornire le metriche di loss e accuracy finali tramite il comando evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c55cb85d-7dd8-4a0c-b8db-9bff3a65eeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998/998 [==============================] - 146s 146ms/step - loss: 0.0518 - accuracy: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05177713930606842, 0.9941093325614929]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM.evaluate(padded_sequences_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
